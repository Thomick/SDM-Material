{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Fitted Q-Learning: Function Approximation for Continuous State Spaces\n",
        "\n",
        "In the previous TP, we studied tabular Q-Learning for discrete MDPs. This method stores a separate Q-value for each state-action pair in a table. However, this approach fails when:\n",
        "\n",
        "1. The state space is continuous (e.g., robot positions, velocities)\n",
        "2. The state space is too large to enumerate (e.g., image observations)\n",
        "\n",
        "Fitted Q-Learning solves this problem using **function approximation**: instead of storing Q-values in a table, we learn a function $\\hat{Q}(s, a; \\mathbf{w})$ parameterized by weights $\\mathbf{w}$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import PolynomialFeatures"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Function Approximation\n",
        "\n",
        "### 1.1 Linear Q-function approximation\n",
        "\n",
        "We approximate the Q-function as a linear combination of features:\n",
        "\n",
        "$\\hat{Q}(s, a; \\mathbf{w}) = \\phi(s, a)^T \\mathbf{w}$\n",
        "\n",
        "where:\n",
        "- $\\phi(s, a)$ is a feature vector extracted from state $s$ and action $a$\n",
        "- $\\mathbf{w}$ are learnable weights\n",
        "\n",
        "### 1.2 Fitted Q-Iteration (FQI)\n",
        "\n",
        "FQI is a batch reinforcement learning algorithm that learns Q-values from a dataset of transitions $\\mathcal{D} = \\{(s_i, a_i, r_i, s'_i)\\}_{i=1}^N$.\n",
        "\n",
        "The algorithm iteratively refines the Q-function approximation:\n",
        "\n",
        "1. Initialize $\\hat{Q}^{(0)}$\n",
        "2. For iteration $k = 1, 2, \\ldots, K$:\n",
        "   - For each transition $(s_i, a_i, r_i, s'_i)$ in $\\mathcal{D}$:\n",
        "     - Compute target: $y_i = r_i + \\gamma \\max_{a'} \\hat{Q}^{(k-1)}(s'_i, a')$\n",
        "   - Update: $\\hat{Q}^{(k)} = \\underset{\\hat{Q}}{\\operatorname{argmin}} \\sum_i (\\hat{Q}(s_i, a_i) - y_i)^2$\n",
        "\n",
        "This converts the RL problem into a sequence of supervised regression problems."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Continuous GridWorld Environment\n",
        "\n",
        "We consider a continuous version of the GridWorld:\n",
        "\n",
        "- **State space**: $s = (x, y) \\in [0, \\text{grid\\_size}]^2$ (continuous)\n",
        "- **Action space**: $a \\in \\{0, 1, 2, 3\\}$ = {up, down, left, right} (discrete)\n",
        "- **Dynamics**: The agent moves $\\delta = 0.5$ in the chosen direction, with Gaussian noise $\\mathcal{N}(0, 0.1^2)$ added to both coordinates\n",
        "- **Rewards**:\n",
        "  - Reaching the goal (distance < 0.3): $+10$ (terminal)\n",
        "  - Each step: $-0.1$\n",
        "- **Initial state**: Near bottom-left corner\n",
        "- **Goal**: Top-right corner"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.1 The Continuous GridWorld environment\n",
        "\n",
        "We provide a complete implementation of the continuous GridWorld. Students will use this environment to test Fitted Q-Learning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ContinuousGridWorld:\n",
        "    def __init__(self, grid_size=5.0, goal_pos=None, noise_std=1.0, step_size=0.5):\n",
        "        self.grid_size = grid_size\n",
        "        self.goal_pos = goal_pos if goal_pos is not None else np.array([grid_size - 0.5, grid_size - 0.5])\n",
        "        self.noise_std = noise_std\n",
        "        self.step_size = step_size\n",
        "        self.goal_radius = 0.3\n",
        "        \n",
        "        # Action mapping: 0=up, 1=down, 2=left, 3=right\n",
        "        self.action_effects = {\n",
        "            0: np.array([0, self.step_size]),   # up\n",
        "            1: np.array([0, -self.step_size]),  # down\n",
        "            2: np.array([-self.step_size, 0]),  # left\n",
        "            3: np.array([self.step_size, 0])    # right\n",
        "        }\n",
        "        \n",
        "        self.current_state = None\n",
        "        self.timestep = 0\n",
        "        self.max_steps = 200\n",
        "    \n",
        "    def reset(self):\n",
        "        \"\"\"Initialize episode with random start position near bottom-left\"\"\"\n",
        "        self.current_state = np.array([0.5, 0.5]) + np.random.randn(2) * 0.2\n",
        "        self.current_state = np.clip(self.current_state, 0, self.grid_size)\n",
        "        self.timestep = 0\n",
        "        return self.current_state.copy()\n",
        "    \n",
        "    def step(self, action):\n",
        "        \"\"\"Execute action and return next state, reward, done\"\"\"\n",
        "        # Apply action with Gaussian noise\n",
        "        noise = np.random.randn(2) * self.noise_std\n",
        "        next_state = self.current_state + self.action_effects[action] + noise\n",
        "        \n",
        "        # Clip to grid boundaries\n",
        "        next_state = np.clip(next_state, 0, self.grid_size)\n",
        "        \n",
        "        # Compute reward and check if done\n",
        "        distance_to_goal = np.linalg.norm(next_state - self.goal_pos)\n",
        "        \n",
        "        if distance_to_goal < self.goal_radius:\n",
        "            reward = 10.0\n",
        "            done = True\n",
        "        elif self.timestep >= self.max_steps:\n",
        "            reward = -0.1\n",
        "            done = True\n",
        "        else:\n",
        "            reward = -0.1\n",
        "            done = False\n",
        "        \n",
        "        self.current_state = next_state\n",
        "        self.timestep += 1\n",
        "        \n",
        "        return next_state.copy(), reward, done\n",
        "    \n",
        "    def render(self, trajectories=None):\n",
        "        \"\"\"Visualize the environment and optional trajectories\"\"\"\n",
        "        plt.figure(figsize=(6, 6))\n",
        "        \n",
        "        # Draw goal\n",
        "        circle = plt.Circle(self.goal_pos, self.goal_radius, color='green', alpha=0.3, label='Goal')\n",
        "        plt.gca().add_patch(circle)\n",
        "        plt.plot(self.goal_pos[0], self.goal_pos[1], 'g*', markersize=15)\n",
        "        \n",
        "        # Draw trajectories if provided\n",
        "        if trajectories is not None:\n",
        "            for traj in trajectories:\n",
        "                states = np.array(traj)\n",
        "                plt.plot(states[:, 0], states[:, 1], 'b-', alpha=0.3, linewidth=0.5)\n",
        "                plt.plot(states[0, 0], states[0, 1], 'ro', markersize=5)\n",
        "        \n",
        "        plt.xlim(-0.2, self.grid_size + 0.2)\n",
        "        plt.ylim(-0.2, self.grid_size + 0.2)\n",
        "        plt.xlabel('x')\n",
        "        plt.ylabel('y')\n",
        "        plt.title('Continuous GridWorld')\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        plt.legend()\n",
        "        plt.axis('equal')\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Test the environment with a random policy:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "env = ContinuousGridWorld()\n",
        "\n",
        "# Collect a few random trajectories\n",
        "trajectories = []\n",
        "for _ in range(5):\n",
        "    traj = []\n",
        "    state = env.reset()\n",
        "    traj.append(state)\n",
        "    done = False\n",
        "    \n",
        "    while not done:\n",
        "        action = np.random.randint(4)\n",
        "        state, reward, done = env.step(action)\n",
        "        traj.append(state)\n",
        "    \n",
        "    trajectories.append(traj)\n",
        "\n",
        "env.render(trajectories)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.2 Why tabular Q-Learning may fails\n",
        "\n",
        "To understand why we need function approximation, let's try discretizing the continuous state space and analyze the coverage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def discretize_state(state, grid_size, n_bins=10):\n",
        "    \"\"\"Discretize continuous state into bins\"\"\"\n",
        "    # TODO: Compute bin indices for state\n",
        "    raise NotImplementedError\n",
        "    return bin_x, bin_y\n",
        "\n",
        "# Collect data with random policy\n",
        "n_bins = 10\n",
        "visit_counts = np.zeros((n_bins, n_bins))\n",
        "\n",
        "for episode in range(50):\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    \n",
        "    while not done:\n",
        "        bin_x, bin_y = discretize_state(state, env.grid_size, n_bins)\n",
        "        visit_counts[bin_x, bin_y] += 1\n",
        "        \n",
        "        action = np.random.randint(4)\n",
        "        state, reward, done = env.step(action)\n",
        "\n",
        "# Visualize state coverage\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.imshow(visit_counts.T, origin='lower', cmap='Blues')\n",
        "plt.colorbar(label='Visit count')\n",
        "plt.xlabel('x bin')\n",
        "plt.ylabel('y bin')\n",
        "plt.title(f'State visitation with {n_bins}x{n_bins} discretization')\n",
        "plt.show()\n",
        "\n",
        "# Print statistics\n",
        "total_bins = n_bins * n_bins\n",
        "visited_bins = np.sum(visit_counts > 0)\n",
        "print(f\"Visited {visited_bins}/{total_bins} bins ({100*visited_bins/total_bins:.1f}%)\")\n",
        "print(f\"Many bins are never visited, so tabular Q-learning would have no data to learn from!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Data Collection\n",
        "\n",
        "Fitted Q-Iteration is a batch RL algorithm: it learns from a fixed dataset of transitions.\n",
        "\n",
        "### 3.1 Collect a dataset of transitions\n",
        "\n",
        "Collect transitions $(s, a, r, s', \\text{done})$ by running episodes with a random policy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def collect_data(env, num_episodes=100):\n",
        "    \"\"\"Collect transitions using random policy\"\"\"\n",
        "    dataset = []\n",
        "    np.random.seed(1)\n",
        "    \n",
        "    for episode in range(num_episodes):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        \n",
        "        while not done:\n",
        "            action = np.random.randint(4)\n",
        "            next_state, reward, done = env.step(action)\n",
        "            \n",
        "            dataset.append((state, action, reward, next_state, done))\n",
        "            state = next_state\n",
        "    \n",
        "    return dataset\n",
        "\n",
        "# Collect data\n",
        "dataset = collect_data(env, num_episodes=50)\n",
        "print(f\"Collected {len(dataset)} transitions\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Visualize the collected data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract states and rewards\n",
        "states = np.array([transition[0] for transition in dataset])\n",
        "rewards = np.array([transition[2] for transition in dataset])\n",
        "\n",
        "# Plot state distribution\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.scatter(states[:, 0], states[:, 1], alpha=0.1, s=1)\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('y')\n",
        "plt.title('Visited states')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.subplot(1, 3, 2)\n",
        "actions = [transition[1] for transition in dataset]\n",
        "plt.hist(actions, bins=4, range=(-0.5, 3.5), edgecolor='black')\n",
        "plt.xlabel('Action')\n",
        "plt.ylabel('Count')\n",
        "plt.title('Action distribution')\n",
        "plt.xticks([0, 1, 2, 3], ['up', 'down', 'left', 'right'])\n",
        "\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.hist(rewards, bins=20, edgecolor='black')\n",
        "plt.xlabel('Reward')\n",
        "plt.ylabel('Count')\n",
        "plt.title('Reward distribution')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Feature Engineering\n",
        "\n",
        "To use linear regression, we need to convert $(s, a)$ pairs into feature vectors $\\phi(s, a)$.\n",
        "\n",
        "### 4.1 Polynomial features for states\n",
        "\n",
        "For a 2D state $s = (x, y)$, polynomial features of degree 2 are:\n",
        "\n",
        "$\\phi_{\\text{state}}(s) = [1, x, y, x^2, xy, y^2]$\n",
        "\n",
        "For degree 3: $[1, x, y, x^2, xy, y^2, x^3, x^2y, xy^2, y^3]$\n",
        "\n",
        "### 4.2 Action encoding\n",
        "\n",
        "We combine state features with one-hot encoded actions. For 4 actions and degree 2 state features (6 dimensions), we get:\n",
        "\n",
        "$\\phi(s, a) = [\\phi_{\\text{state}}(s) \\text{ if } a=0 \\text{ else } \\mathbf{0}, \\ldots, \\phi_{\\text{state}}(s) \\text{ if } a=3 \\text{ else } \\mathbf{0}]$\n",
        "\n",
        "This creates a $6 \\times 4 = 24$ dimensional feature vector."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.3 Exercise : Implement feature extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class FeatureExtractor:\n",
        "    def __init__(self, degree=2, n_actions=4):\n",
        "        self.degree = degree\n",
        "        self.n_actions = n_actions\n",
        "        self.poly = PolynomialFeatures(degree=degree)\n",
        "        \n",
        "        # Fit on dummy data to initialize\n",
        "        dummy = np.array([[0, 0]])\n",
        "        self.poly.fit(dummy)\n",
        "        self.n_state_features = self.poly.transform(dummy).shape[1]\n",
        "    \n",
        "    def extract(self, state, action):\n",
        "        \"\"\"Extract feature vector for (state, action) pair\"\"\"\n",
        "        # TODO: Compute polynomial features for state\n",
        "        # TODO: Create feature vector with state features at action index\n",
        "        # Hint: Create vector of size (n_state_features * n_actions)\n",
        "        raise NotImplementedError\n",
        "    \n",
        "    def extract_batch(self, states, actions):\n",
        "        \"\"\"Extract features for batch of (state, action) pairs\"\"\"\n",
        "        return np.array([self.extract(s, a) for s, a in zip(states, actions)])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Test the feature extractor:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "feat = FeatureExtractor(degree=2)\n",
        "test_state = np.array([1.0, 2.0])\n",
        "test_action = 0\n",
        "features = feat.extract(test_state, test_action)\n",
        "print(f\"Feature vector shape: {features.shape}\")\n",
        "print(f\"Number of features: {len(features)} (= {feat.n_state_features} state features x {feat.n_actions} actions)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Fitted Q-Iteration\n",
        "\n",
        "### 5.1 Exercise : Implement the FQI algorithm\n",
        "\n",
        "The algorithm:\n",
        "\n",
        "1. Extract features for all $(s, a)$ pairs in the dataset\n",
        "2. Initialize Q-function (linear regression model)\n",
        "3. For each iteration:\n",
        "   - Compute target values: $y_i = r_i + \\gamma \\max_{a'} \\hat{Q}(s'_i, a')$\n",
        "   - Fit Q-function to predict targets from features\n",
        "4. Return learned Q-function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class QFunction:\n",
        "    def __init__(self, feature_extractor):\n",
        "        self.feat = feature_extractor\n",
        "        self.model = LinearRegression()\n",
        "        self.is_fitted = False\n",
        "    \n",
        "    def predict(self, state, action):\n",
        "        \"\"\"Predict Q-value for (state, action)\"\"\"\n",
        "        if not self.is_fitted:\n",
        "            return 0.0\n",
        "        features = self.feat.extract(state, action).reshape(1, -1)\n",
        "        return self.model.predict(features)[0]\n",
        "    \n",
        "    def predict_batch(self, states, actions):\n",
        "        \"\"\"Predict Q-values for batch\"\"\"\n",
        "        if not self.is_fitted:\n",
        "            return np.zeros(len(states))\n",
        "        features = self.feat.extract_batch(states, actions)\n",
        "        return self.model.predict(features)\n",
        "    \n",
        "    def max_q(self, state):\n",
        "        \"\"\"Compute max_a Q(state, a)\"\"\"\n",
        "        q_values = [self.predict(state, a) for a in range(self.feat.n_actions)]\n",
        "        return np.max(q_values)\n",
        "    \n",
        "    def greedy_action(self, state):\n",
        "        \"\"\"Return argmax_a Q(state, a)\"\"\"\n",
        "        q_values = [self.predict(state, a) for a in range(self.feat.n_actions)]\n",
        "        return np.argmax(q_values)\n",
        "    \n",
        "    def fit(self, features, targets):\n",
        "        \"\"\"Fit Q-function to targets\"\"\"\n",
        "        self.model.fit(features, targets)\n",
        "        self.is_fitted = True\n",
        "\n",
        "def fitted_q_iteration(dataset, degree=2, gamma=0.95, num_iterations=20):\n",
        "    \"\"\"Fitted Q-Iteration algorithm\"\"\"\n",
        "    # Initialize feature extractor and Q-function\n",
        "    feat = FeatureExtractor(degree=degree)\n",
        "    q_func = QFunction(feat)\n",
        "    \n",
        "    # Extract components from dataset\n",
        "    states = np.array([t[0] for t in dataset])\n",
        "    actions = np.array([t[1] for t in dataset])\n",
        "    rewards = np.array([t[2] for t in dataset])\n",
        "    next_states = np.array([t[3] for t in dataset])\n",
        "    dones = np.array([t[4] for t in dataset])\n",
        "    \n",
        "    # Extract features for all (s, a) pairs\n",
        "    features = feat.extract_batch(states, actions)\n",
        "    \n",
        "    # FQI iterations\n",
        "    for iteration in range(num_iterations):\n",
        "        # TODO: Compute targets for each transition\n",
        "        targets = None\n",
        "        \n",
        "        # TODO: Fit Q-function\n",
        "        raise NotImplementedError\n",
        "        \n",
        "        if iteration % 5 == 0:\n",
        "            mean_target = np.mean(targets)\n",
        "            print(f\"Iteration {iteration}: mean target = {mean_target:.3f}\")\n",
        "    \n",
        "    return q_func"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run Fitted Q-Iteration:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "q_func = fitted_q_iteration(dataset, degree=3, gamma=0.95, num_iterations=20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Policy Evaluation\n",
        "\n",
        "### 6.1 Exercise : Extract and evaluate the learned policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_policy(env, policy_fn, num_episodes=50, render_episodes=0):\n",
        "    \"\"\"Evaluate policy and return statistics\"\"\"\n",
        "    scores = []\n",
        "    successes = 0\n",
        "    trajectories = []\n",
        "    \n",
        "    for episode in range(num_episodes):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        episode_reward = 0\n",
        "        traj = [state.copy()]\n",
        "        \n",
        "        # TODO: Run one episode following policy_fn\n",
        "        # - Use policy_fn(state) to select actions\n",
        "        # - Accumulate rewards in episode_reward\n",
        "        # - Store states in traj\n",
        "        raise NotImplementedError\n",
        "        \n",
        "        scores.append(episode_reward)\n",
        "        if reward > 5:  # Reached goal\n",
        "            successes += 1\n",
        "        \n",
        "        if episode < render_episodes:\n",
        "            trajectories.append(traj)\n",
        "    \n",
        "    return {\n",
        "        'mean_score': np.mean(scores),\n",
        "        'std_score': np.std(scores),\n",
        "        'success_rate': successes / num_episodes,\n",
        "        'trajectories': trajectories\n",
        "    }\n",
        "\n",
        "# Evaluate learned policy\n",
        "learned_policy = lambda s: q_func.greedy_action(s)\n",
        "results_learned = evaluate_policy(env, learned_policy, num_episodes=50, render_episodes=3)\n",
        "\n",
        "# Compare with random policy\n",
        "random_policy = lambda s: np.random.randint(4)\n",
        "results_random = evaluate_policy(env, random_policy, num_episodes=50)\n",
        "\n",
        "print(\"Learned policy:\")\n",
        "print(f\"  Mean score: {results_learned['mean_score']:.2f} +/- {results_learned['std_score']:.2f}\")\n",
        "print(f\"  Success rate: {results_learned['success_rate']:.1%}\")\n",
        "\n",
        "print(\"\\nRandom policy:\")\n",
        "print(f\"  Mean score: {results_random['mean_score']:.2f} +/- {results_random['std_score']:.2f}\")\n",
        "print(f\"  Success rate: {results_random['success_rate']:.1%}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Visualize learned policy trajectories:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "env.render(results_learned['trajectories'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.2 Exercise : Visualize the learned Q-function\n",
        "\n",
        "Plot heatmaps of $\\hat{Q}(s, a)$ for each action."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def visualize_q_function(q_func, grid_size=5.0, resolution=50):\n",
        "    \"\"\"Plot Q-function heatmaps for each action\"\"\"\n",
        "    # Create grid of states\n",
        "    x = np.linspace(0, grid_size, resolution)\n",
        "    y = np.linspace(0, grid_size, resolution)\n",
        "    X, Y = np.meshgrid(x, y)\n",
        "    \n",
        "    action_names = ['Up', 'Down', 'Left', 'Right']\n",
        "    \n",
        "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
        "    axes = axes.flatten()\n",
        "    \n",
        "    # TODO: For each action (0-3):\n",
        "    # 1. Create Q_values array of shape (resolution, resolution)\n",
        "    # 2. For each grid position (i, j):\n",
        "    #    - Create state from X[i, j], Y[i, j]\n",
        "    #    - Compute Q_values[i, j]\n",
        "    # 3. Plot using axes[action].imshow() with extent=[0, grid_size, 0, grid_size]\n",
        "    # 4. To mark the goal you can use axes[action].plot(env.goal_pos[0], env.goal_pos[1], 'r*', markersize=15)\n",
        "    raise NotImplementedError\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "visualize_q_function(q_func)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Visualize the policy:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def visualize_policy(q_func, grid_size=5.0, resolution=100):\n",
        "    \"\"\"Plot policy as color-coded heatmap\"\"\"\n",
        "    x = np.linspace(0, grid_size, resolution)\n",
        "    y = np.linspace(0, grid_size, resolution)\n",
        "    X, Y = np.meshgrid(x, y)\n",
        "    \n",
        "    # Compute best action at each grid point\n",
        "    policy_map = np.zeros((resolution, resolution))\n",
        "    for i in range(resolution):\n",
        "        for j in range(resolution):\n",
        "            state = np.array([X[i, j], Y[i, j]])\n",
        "            policy_map[i, j] = q_func.greedy_action(state)\n",
        "    \n",
        "    # Color map: 0=up (blue), 1=down (orange), 2=left (green), 3=right (red)\n",
        "    plt.figure(figsize=(8, 7))\n",
        "    im = plt.imshow(policy_map, origin='lower', extent=[0, grid_size, 0, grid_size],\n",
        "                    cmap='tab10', vmin=0, vmax=3, aspect='auto')\n",
        "    \n",
        "    # Add colorbar with action labels\n",
        "    cbar = plt.colorbar(im, ticks=[0, 1, 2, 3])\n",
        "    cbar.set_label('Action', rotation=270, labelpad=20)\n",
        "    cbar.ax.set_yticklabels(['Up', 'Down', 'Left', 'Right'])\n",
        "    \n",
        "    # Mark goal\n",
        "    circle = plt.Circle(env.goal_pos, env.goal_radius, fill=False, \n",
        "                       edgecolor='white', linewidth=3, label='Goal')\n",
        "    plt.gca().add_patch(circle)\n",
        "    plt.plot(env.goal_pos[0], env.goal_pos[1], 'w*', markersize=20)\n",
        "    \n",
        "    plt.xlabel('x')\n",
        "    plt.ylabel('y')\n",
        "    plt.title('Learned Policy (Fitted Q-Learning)')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "visualize_policy(q_func)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Comparison with Discretization + Tabular Q-Learning\n",
        "\n",
        "To understand the benefit of function approximation, let's compare with a naive discretization approach using tabular Q-Learning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def discretize_continuous_state(state, grid_size, n_bins):\n",
        "    \"\"\"Map continuous state to discrete bin index\"\"\"\n",
        "    bin_size = grid_size / n_bins\n",
        "    bin_x = int(np.clip(state[0] / bin_size, 0, n_bins - 1))\n",
        "    bin_y = int(np.clip(state[1] / bin_size, 0, n_bins - 1))\n",
        "    return bin_x * n_bins + bin_y\n",
        "\n",
        "def q_iteration_tabular(dataset, grid_size, n_bins=20, gamma=0.95, num_iterations=20):\n",
        "    \"\"\"Q-Iteration with discretization (batch learning from same dataset)\"\"\"\n",
        "    n_states = n_bins * n_bins\n",
        "    n_actions = 4\n",
        "    Q = np.zeros((n_states, n_actions))\n",
        "    \n",
        "    # Extract and discretize transitions\n",
        "    transitions = []\n",
        "    for state, action, reward, next_state, done in dataset:\n",
        "        s_idx = discretize_continuous_state(state, grid_size, n_bins)\n",
        "        s_next_idx = discretize_continuous_state(next_state, grid_size, n_bins)\n",
        "        transitions.append((s_idx, action, reward, s_next_idx, done))\n",
        "    \n",
        "    # Q-Iteration updates\n",
        "    for iteration in range(num_iterations):\n",
        "        Q_old = Q.copy()\n",
        "        \n",
        "        for s_idx, action, reward, s_next_idx, done in transitions:\n",
        "            if done:\n",
        "                target = reward\n",
        "            else:\n",
        "                target = reward + gamma * np.max(Q_old[s_next_idx, :])\n",
        "            \n",
        "            Q[s_idx, action] = target\n",
        "        \n",
        "        if iteration % 5 == 0:\n",
        "            mean_q = np.mean(Q[Q != 0])  # Mean of visited states\n",
        "            print(f\"Iteration {iteration}: mean Q-value = {mean_q:.3f}\")\n",
        "    \n",
        "    return Q, n_bins\n",
        "\n",
        "# Train tabular Fitted Q-Iteration using the SAME dataset as function approximation\n",
        "print(\"Training tabular Q-Learning with discretization (using same dataset)...\")\n",
        "Q_tabular, n_bins = fitted_q_iteration_tabular(dataset, env.grid_size, n_bins=20, num_iterations=20)\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create a policy from the tabular Q-function and evaluate it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def tabular_policy(state, Q_table, grid_size, n_bins):\n",
        "    \"\"\"Extract action from tabular Q-function\"\"\"\n",
        "    # TODO: Compute state index and return argmax action\n",
        "    raise NotImplementedError\n",
        "\n",
        "# Evaluate tabular policy\n",
        "tabular_pol = lambda s: tabular_policy(s, Q_tabular, env.grid_size, n_bins)\n",
        "results_tabular = evaluate_policy(env, tabular_pol, num_episodes=50)\n",
        "\n",
        "print(\"Tabular Q-Learning (discretized):\")\n",
        "print(f\"  Mean score: {results_tabular['mean_score']:.2f} +/- {results_tabular['std_score']:.2f}\")\n",
        "print(f\"  Success rate: {results_tabular['success_rate']:.1%}\")\n",
        "\n",
        "print(\"\\nFitted Q-Learning (function approximation):\")\n",
        "print(f\"  Mean score: {results_learned['mean_score']:.2f} +/- {results_learned['std_score']:.2f}\")\n",
        "print(f\"  Success rate: {results_learned['success_rate']:.1%}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Visualize the discretized policy:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def visualize_discretized_policy(Q_table, n_bins, grid_size=5.0):\n",
        "    \"\"\"Visualize policy learned with discretization\"\"\"\n",
        "    policy_map = np.zeros((n_bins, n_bins))\n",
        "    \n",
        "    for i in range(n_bins):\n",
        "        for j in range(n_bins):\n",
        "            state_idx = i * n_bins + j\n",
        "            policy_map[i, j] = np.argmax(Q_table[state_idx, :])\n",
        "    \n",
        "    plt.figure(figsize=(8, 7))\n",
        "    im = plt.imshow(policy_map, origin='lower', extent=[0, grid_size, 0, grid_size],\n",
        "                    cmap='tab10', vmin=0, vmax=3, aspect='auto', interpolation='nearest')\n",
        "    \n",
        "    cbar = plt.colorbar(im, ticks=[0, 1, 2, 3])\n",
        "    cbar.set_label('Action', rotation=270, labelpad=20)\n",
        "    cbar.ax.set_yticklabels(['Up', 'Down', 'Left', 'Right'])\n",
        "    \n",
        "    # Mark goal\n",
        "    circle = plt.Circle(env.goal_pos, env.goal_radius, fill=False,\n",
        "                       edgecolor='white', linewidth=3, label='Goal')\n",
        "    plt.gca().add_patch(circle)\n",
        "    plt.plot(env.goal_pos[0], env.goal_pos[1], 'w*', markersize=20)\n",
        "    \n",
        "    # Draw grid lines to show discretization\n",
        "    for i in range(n_bins + 1):\n",
        "        val = i * grid_size / n_bins\n",
        "        plt.axhline(val, color='gray', linewidth=0.5, alpha=0.5)\n",
        "        plt.axvline(val, color='gray', linewidth=0.5, alpha=0.5)\n",
        "    \n",
        "    plt.xlabel('x')\n",
        "    plt.ylabel('y')\n",
        "    plt.title(f'Discretized Policy (Tabular Q-Learning, {n_bins}x{n_bins} bins)')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "visualize_discretized_policy(Q_tabular, n_bins)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Policy Gradient (REINFORCE)\n",
        "\n",
        "Unlike value-based methods (Q-learning, Fitted Q-Iteration), **policy gradient methods** directly optimize a parametric policy $\\pi_\\theta(a|s)$ without explicitly learning a value function.\n",
        "\n",
        "### 8.1 Softmax Policy Parameterization\n",
        "\n",
        "We use a **softmax policy** with polynomial features:\n",
        "\n",
        "$$\\pi_\\theta(a|s) = \\frac{\\exp(\\theta_a^\\top \\phi(s))}{\\sum_{a'=0}^{n_a-1} \\exp(\\theta_{a'}^\\top \\phi(s))}$$\n",
        "\n",
        "where:\n",
        "- $\\theta = [\\theta_0, \\theta_1, \\theta_2, \\theta_3]$ are the parameters (one weight vector per action)\n",
        "- $\\phi(s)$ are polynomial features of the state (same as for Fitted Q-Iteration)\n",
        "- The softmax ensures $\\sum_a \\pi_\\theta(a|s) = 1$ and $\\pi_\\theta(a|s) \\geq 0$\n",
        "\n",
        "### 8.2 Policy Gradient Theorem\n",
        "\n",
        "The gradient of the expected return $J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta}[R(\\tau)]$ is:\n",
        "\n",
        "$$\\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta}\\left[\\sum_{t=0}^T \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t) \\cdot R_t\\right]$$\n",
        "\n",
        "where $R_t = \\sum_{t'=t}^T \\gamma^{t'-t} r_{t'}$ is the **discounted return-to-go** from timestep $t$.\n",
        "\n",
        "**Key insight**: We increase the probability of actions that led to high returns, and decrease the probability of actions that led to low returns.\n",
        "\n",
        "### 8.3 Variance Reduction Techniques\n",
        "\n",
        "**Baseline subtraction**: Instead of using $R_t$ directly, we use **advantages** $A_t = R_t - b$ where $b$ is a baseline (e.g., moving average of returns). This reduces variance without introducing bias.\n",
        "\n",
        "**Advantage normalization**: We normalize advantages to have zero mean and unit variance within each episode. This stabilizes learning across different scales of rewards.\n",
        "\n",
        "### 8.4 Bootstrap Learning\n",
        "\n",
        "With sparse rewards, pure exploration can take very long to find successful trajectories. We **bootstrap** the policy by:\n",
        "1. Collecting successful trajectories using a random policy\n",
        "2. Using these trajectories to initialize the policy parameters\n",
        "3. Then continuing with online learning\n",
        "\n",
        "This provides an initial learning signal and speeds up convergence.\n",
        "\n",
        "### 8.5 REINFORCE Algorithm\n",
        "\n",
        "The REINFORCE algorithm:\n",
        "1. Collect a trajectory by following $\\pi_\\theta$\n",
        "2. Compute returns $R_t$ for each timestep\n",
        "3. Compute advantages $A_t = R_t - \\text{baseline}$\n",
        "4. Update policy: $\\theta \\leftarrow \\theta + \\alpha \\sum_t A_t \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t)$\n",
        "5. Repeat"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 8.6 Exercise: Implement REINFORCE\n",
        "\n",
        "Implement the REINFORCE algorithm with baseline and bootstrap learning.\n",
        "\n",
        "**Implementation notes**:\n",
        "- The gradient of $\\log \\pi_\\theta(a|s)$ for softmax policy is: $\\nabla_\\theta \\log \\pi_\\theta(a|s) = \\phi(s) \\cdot (\\mathbf{1}_{a} - \\pi_\\theta(s))$\n",
        "  - where $\\mathbf{1}_a$ is a one-hot vector (1 for the selected action, 0 otherwise)\n",
        "  - and $\\pi_\\theta(s)$ is the vector of action probabilities\n",
        "- For bootstrapping, update policy parameters using successful trajectories before starting online learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def collect_initial_trajectories(env, n_episodes=50):\n",
        "    \"\"\"Collect successful trajectories using random policy to bootstrap learning\"\"\"\n",
        "    trajectories = []\n",
        "    for _ in range(n_episodes):\n",
        "        states = []\n",
        "        actions = []\n",
        "        rewards = []\n",
        "        \n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        \n",
        "        while not done:\n",
        "            action = np.random.randint(4)\n",
        "            next_state, reward, done = env.step(action)\n",
        "            \n",
        "            states.append(state)\n",
        "            actions.append(action)\n",
        "            rewards.append(reward)\n",
        "            \n",
        "            state = next_state\n",
        "        \n",
        "        # Only keep successful trajectories (reward > 0 indicates success)\n",
        "        if reward > 0:\n",
        "            trajectories.append((states, actions, rewards))\n",
        "    \n",
        "    return trajectories\n",
        "\n",
        "class SoftmaxPolicy:\n",
        "    \"\"\"Softmax policy with linear features\"\"\"\n",
        "    def __init__(self, n_actions=4, state_degree=2):\n",
        "        self.n_actions = n_actions\n",
        "        self.poly = PolynomialFeatures(degree=state_degree)\n",
        "        self.poly.fit(np.array([[0, 0]]))\n",
        "        self.n_features = self.poly.transform(np.array([[0, 0]])).shape[1]\n",
        "        \n",
        "        self.theta = np.random.randn(n_actions, self.n_features) * 0.01\n",
        "    \n",
        "    def features(self, state):\n",
        "        return self.poly.transform(state.reshape(1, -1)).flatten()\n",
        "    \n",
        "    def action_probabilities(self, state):\n",
        "        phi = self.features(state)\n",
        "        logits = self.theta @ phi\n",
        "        # Numerical stability: subtract max\n",
        "        logits = logits - np.max(logits)\n",
        "        exp_logits = np.exp(logits)\n",
        "        return exp_logits / np.sum(exp_logits)\n",
        "    \n",
        "    def sample_action(self, state):\n",
        "        probs = self.action_probabilities(state)\n",
        "        return np.random.choice(self.n_actions, p=probs)\n",
        "    \n",
        "    def greedy_action(self, state):\n",
        "        probs = self.action_probabilities(state)\n",
        "        return np.argmax(probs)\n",
        "\n",
        "def compute_returns(rewards, gamma=0.95):\n",
        "    returns = np.zeros(len(rewards))\n",
        "    running_return = 0\n",
        "    for t in reversed(range(len(rewards))):\n",
        "        running_return = rewards[t] + gamma * running_return\n",
        "        returns[t] = running_return\n",
        "    return returns\n",
        "\n",
        "def reinforce(env, n_actions=4, state_degree=2, \n",
        "             gamma=0.95, lr=0.001, num_episodes=500, initial_trajectories=None):\n",
        "    \"\"\"REINFORCE algorithm: online policy gradient with baseline\"\"\"\n",
        "    policy = SoftmaxPolicy(n_actions, state_degree)\n",
        "    episode_returns = []\n",
        "    \n",
        "    # Bootstrap with initial successful trajectories if provided\n",
        "    if initial_trajectories:\n",
        "        print(f\"Bootstrapping with {len(initial_trajectories)} successful trajectories...\")\n",
        "        for states, actions, rewards in initial_trajectories:\n",
        "            returns = compute_returns(rewards, gamma)\n",
        "            \n",
        "            for t in range(len(states)):\n",
        "                state = states[t]\n",
        "                action = actions[t]\n",
        "                R_t = returns[t]\n",
        "                \n",
        "                phi = policy.features(state)\n",
        "                probs = policy.action_probabilities(state)\n",
        "                \n",
        "                # TODO Exercise 5a: Compute gradient of log pi(a|s)\n",
        "                # Hint: For the action taken, grad = phi * (1 - prob[action])\n",
        "                #       For other actions, grad = -phi * prob[action]\n",
        "                grad_log_pi = np.zeros_like(policy.theta)\n",
        "                raise NotImplementedError\n",
        "                \n",
        "                policy.theta += lr * R_t * grad_log_pi\n",
        "    \n",
        "    for episode in range(num_episodes):\n",
        "        # Collect trajectory using current policy\n",
        "        states = []\n",
        "        actions = []\n",
        "        rewards = []\n",
        "        \n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        \n",
        "        while not done:\n",
        "            action = policy.sample_action(state)\n",
        "            next_state, reward, done = env.step(action)\n",
        "            \n",
        "            states.append(state)\n",
        "            actions.append(action)\n",
        "            rewards.append(reward)\n",
        "            \n",
        "            state = next_state\n",
        "        \n",
        "        # Compute returns\n",
        "        returns = compute_returns(rewards, gamma)\n",
        "        episode_returns.append(sum(rewards))\n",
        "        \n",
        "        # TODO Exercise 5b: Compute baseline and advantages\n",
        "        # Baseline: mean of last 50 episode returns (or 0 if < 10 episodes)\n",
        "        # Advantages: returns - baseline, then normalize if len > 1\n",
        "        baseline = None\n",
        "        advantages = None\n",
        "        raise NotImplementedError\n",
        "        \n",
        "        # TODO Exercise 5c: Update policy using policy gradient\n",
        "        # For each timestep, compute grad_log_pi (same as bootstrap above)\n",
        "        # and update: policy.theta += lr * advantage[t] * grad_log_pi\n",
        "        raise NotImplementedError\n",
        "        \n",
        "        if episode % 100 == 0 and episode > 0:\n",
        "            avg_return = np.mean(episode_returns[-100:])\n",
        "            print(f\"Episode {episode}: avg return (last 100) = {avg_return:.3f}\")\n",
        "    \n",
        "    return policy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Collect initial successful trajectories and train the policy gradient method:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Collect initial successful trajectories using random policy\n",
        "initial_trajs = collect_initial_trajectories(env, n_episodes=50)\n",
        "print(f\"Collected {len(initial_trajs)} successful trajectories\")\n",
        "\n",
        "# Train REINFORCE with bootstrapping\n",
        "pg_policy = reinforce(env, state_degree=2, lr=0.001, num_episodes=500, initial_trajectories=initial_trajs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Evaluate the policy gradient policy:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate policy gradient policy\n",
        "pg_greedy = lambda s: pg_policy.greedy_action(s)\n",
        "results_pg = evaluate_policy(env, pg_greedy, num_episodes=50, render_episodes=3)\n",
        "\n",
        "print(\"Policy Gradient:\")\n",
        "print(f\"  Mean score: {results_pg['mean_score']:.2f} +/- {results_pg['std_score']:.2f}\")\n",
        "print(f\"  Success rate: {results_pg['success_rate']:.1%}\")\n",
        "\n",
        "print(\"\\nComparison:\")\n",
        "print(f\"  Fitted Q-Learning:  {results_learned['success_rate']:.1%} success\")\n",
        "print(f\"  Tabular (discrete): {results_tabular['success_rate']:.1%} success\")\n",
        "print(f\"  Policy Gradient:    {results_pg['success_rate']:.1%} success\")\n",
        "print(f\"  Random:             {results_random['success_rate']:.1%} success\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Visualize policy gradient trajectories:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "env.render(results_pg['trajectories'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Visualize the policy gradient policy:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def visualize_pg_policy(policy, grid_size=5.0, resolution=100):\n",
        "    \"\"\"Visualize policy gradient policy as heatmap\"\"\"\n",
        "    x = np.linspace(0, grid_size, resolution)\n",
        "    y = np.linspace(0, grid_size, resolution)\n",
        "    X, Y = np.meshgrid(x, y)\n",
        "    \n",
        "    policy_map = np.zeros((resolution, resolution))\n",
        "    for i in range(resolution):\n",
        "        for j in range(resolution):\n",
        "            state = np.array([X[i, j], Y[i, j]])\n",
        "            policy_map[i, j] = policy.greedy_action(state)\n",
        "    \n",
        "    plt.figure(figsize=(8, 7))\n",
        "    im = plt.imshow(policy_map, origin='lower', extent=[0, grid_size, 0, grid_size],\n",
        "                    cmap='tab10', vmin=0, vmax=3, aspect='auto')\n",
        "    \n",
        "    cbar = plt.colorbar(im, ticks=[0, 1, 2, 3])\n",
        "    cbar.set_label('Action', rotation=270, labelpad=20)\n",
        "    cbar.ax.set_yticklabels(['Up', 'Down', 'Left', 'Right'])\n",
        "    \n",
        "    circle = plt.Circle(env.goal_pos, env.goal_radius, fill=False,\n",
        "                       edgecolor='white', linewidth=3, label='Goal')\n",
        "    plt.gca().add_patch(circle)\n",
        "    plt.plot(env.goal_pos[0], env.goal_pos[1], 'w*', markersize=20)\n",
        "    \n",
        "    plt.xlabel('x')\n",
        "    plt.ylabel('y')\n",
        "    plt.title('Learned Policy (Policy Gradient)')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "visualize_pg_policy(pg_policy)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
