{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Q68OSoASm9w"
   },
   "source": [
    "# 1. Markov Decision Processes (MDP) $\\langle S, A, R, P, P_0, H, \\gamma\\rangle$\n",
    "\n",
    "$S$ a set of states.\\\n",
    "$A$ a set of actions.\\\n",
    "$R$ a reward mapping : $S \\times A \\rightarrow \\mathbb{R}$.\\\n",
    "$P$ the transition probability distribution $S \\times A \\rightarrow \\Delta S$.\\\n",
    "$P_0$ the initial distribution over states: $s_0 \\in S \\sim P_0$.\\\n",
    "$H$ the horizon: the number of transition per episode.\\\n",
    "$\\gamma$ the discount factor in $(0,1)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2duHvG0tXLFq"
   },
   "source": [
    "MDPs are used to model sequential decision tasks in which at each time step $t$, an agent has to take an action given its current state. From $t=0$ to $t=H$, an agent observes state $s_t$ and takes action $a_t$. The agent then transitions to $s_{t+1}$ with probability $P(s_{t+1}|s_t, a_t)$ and receives reward $r_t = R(s_t,a_t,s_{t+1})$. At $t=0$, the agent \"spawns\" in an initial state $s_0$ with probability $P_0(s_0)$. One can also define terminal states that are such that an agent will not take actions in those states: in such terminal states the MDP stops even though $t<H$. An episode or a trajectory in a MDP, is a sequence of length at most $H$ of transition tuples ($s_t, a_t, r_t, s_{t+1}$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xJ3RmlC8d472"
   },
   "source": [
    "With Dynamic Programming or Reinforcement Learning, it is possible to train agents in MDPs in order to learn good solutions for sequential decision tasks. Such solutions are called policies and are mappings from actions to distributions over actions: $\\Pi: S \\rightarrow \\Delta A$. The goal of Dynamic Programming and RL is to find the optimal policy $\\pi^* = \\underset{\\pi \\in \\Pi}{\\operatorname{argmax}} \\underset{s_0 \\sim P_0,a_t \\sim \\pi(s_t),s_{t+1}\\sim P(s_t, a_t)}{\\mathbb{E}}\\left [ \\underset{t=0}{\\overset{H}{\\sum}}\\gamma^t R(s_t, a_t)\\right ]$. The latter objective function finds policies that maximize the expected cumulative reward of the MDP: we want to find a policy that will, in expectation, generate MDP episodes with high rewards!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kYSbx4kZZa6i"
   },
   "source": [
    "## 1.1 Exercise 1: Implement a Markov Decision Process class\n",
    "\n",
    "In the exercises, we will consider that states and actions are finite: $|S| < \\infty, |A| < \\infty $. For such discrete MDPs, the optimal policy is deterministic i.e $\\pi^*: S \\rightarrow A$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EvZgNWOo-UWj"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IYGbBEW9aar-"
   },
   "outputs": [],
   "source": [
    "class Mdp:\n",
    "    \"\"\"\n",
    "    Defines a Markov Decision Process\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        states: list,\n",
    "        actions: list,\n",
    "        initial_distribution: np.array,  # P0\n",
    "        transition_probability: np.array,  # P\n",
    "        reward_function: np.array,  # R\n",
    "        gamma: float = 0.9,\n",
    "        terminal_states: list = [],\n",
    "        horizon: int = 50,\n",
    "    ):\n",
    "        self.states = states\n",
    "        self.nb_states = len(states)\n",
    "        self.terminal_states = terminal_states\n",
    "\n",
    "        self.actions = actions\n",
    "        self.nb_actions = len(actions)\n",
    "\n",
    "        self.P = transition_probability\n",
    "        self.P0 = initial_distribution\n",
    "\n",
    "        self.R = reward_function\n",
    "\n",
    "        self.horizon = horizon\n",
    "        self.gamma = gamma\n",
    "\n",
    "        self.timestep = 0\n",
    "        self.current_state = None\n",
    "\n",
    "        assert self.check_P_is_distrib(), \"transition matrix is not a valid distribution\"\n",
    "        assert self.check_P0_is_distrib(), \"initial distribution is not valid\"\n",
    "\n",
    "    def check_P_is_distrib(self):\n",
    "        \"\"\"Check that P sums to 1 over next states for all (s,a) pairs\"\"\"\n",
    "        # TODO: Check values are in [0, 1] and sum to 1 for each (s, a)\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def check_P0_is_distrib(self):\n",
    "        \"\"\"Check that P0 sums to 1\"\"\"\n",
    "        # TODO: Check P0 is a valid probability distribution\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Initialize an episode and return the initial state\"\"\"\n",
    "        self.current_state = np.random.choice(a=self.states, p=self.P0)\n",
    "        self.timestep = 0\n",
    "        return self.current_state\n",
    "\n",
    "    def done(self):\n",
    "        \"\"\"Check if episode is over\"\"\"\n",
    "        # TODO: Episode is over if in terminal state or horizon reached\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def step(self, a):\n",
    "        \"\"\"Perform a transition in the MDP\"\"\"\n",
    "        # TODO: Sample next state from P[current_state, a, :]\n",
    "        next_state = None\n",
    "        # TODO: Get reward from R[current_state, a, next_state]\n",
    "        reward = None\n",
    "\n",
    "        self.timestep += 1\n",
    "        self.current_state = next_state\n",
    "        done = self.done()\n",
    "\n",
    "        return next_state, reward, done\n",
    "\n",
    "    def one_episode(self, policy):\n",
    "        \"\"\"Run one episode following a given policy\"\"\"\n",
    "        state = self.reset()\n",
    "        done = False\n",
    "        score = 0\n",
    "        while not done:\n",
    "            action = policy(state)\n",
    "            next_state, r, done = self.step(action)\n",
    "            score += r\n",
    "            state = next_state\n",
    "        return score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lWsZsN8LmF_M"
   },
   "source": [
    "## 1.2 Exercise 2: Model the blackjack with dice game using your MDP class and evaluate the performances of simple policies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mipbo3TFnWJQ"
   },
   "source": [
    "In blackjack with a die, the goal is to throw as many dice as one wants, sequentially, in order to obtain a sum of dice as close as possible to 21. Take your time to model the problem.\n",
    "\n",
    "An episode, i.e a single blackjack with a die game, looks like this: agent throws a die, checks the result, decides to throw again or not, if it threw, checks the sum of dice and repeat, else, the game is over. The score of the agent is the final sum of the dice (or 0, if the sum is strictly greater than 21).\n",
    "\n",
    "- What should the states be?\n",
    "- What does the probability transition matrix look like?\n",
    "- Are there terminal transitions?\n",
    "- What is the reward at time $t$ if the agent score is the *final* sum of dice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wNiltMIwnVWH"
   },
   "outputs": [],
   "source": [
    "### BLACKJACK WITH DICE ###\n",
    "S = np.arange(23)  # s=0: initial, s=1..21: dice sums, s=22: terminal (bust or stop)\n",
    "A = [0, 1]  # 0: stop, 1: throw\n",
    "\n",
    "# Initial distribution: always start at s=0\n",
    "P0 = np.zeros(len(S))\n",
    "P0[0] = 1\n",
    "\n",
    "# TODO: Define the transition probabilities P[s, a, s_next]\n",
    "# Hint: P should be a (23, 2, 23) array\n",
    "# - When a=0 (stop): transition to terminal state (s=22)\n",
    "# - When a=1 (throw): add dice value (1-6) to current sum\n",
    "#   - If sum > 21: go to terminal state\n",
    "#   - Otherwise: go to new sum state\n",
    "P = None  # Replace with your implementation\n",
    "\n",
    "# TODO: Define rewards R[s, a, s_next]\n",
    "# Hint: R should be a (23, 2, 23) array\n",
    "# - When stopping (a=0): reward = current sum (unless bust)\n",
    "# - When throwing (a=1): no immediate reward\n",
    "R = None  # Replace with your implementation\n",
    "\n",
    "# Uncomment when P and R are implemented:\n",
    "# blackjack_dice = Mdp(S, A, P0, P, R, terminal_states=[22])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CnSPmuENS5Ri"
   },
   "source": [
    "A simple class for policies (a policy $\\pi$ takes a state $s$ and returns an action $a$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MpXVDnFxvJPT"
   },
   "outputs": [],
   "source": [
    "class Policy:\n",
    "    def __init__(self, mdp:Mdp):\n",
    "        self.mdp = mdp\n",
    "\n",
    "    def get_action(self, state):\n",
    "        raise NotImplementedError\n",
    "\n",
    "class Uniform(Policy):\n",
    "    def __init__(self, mdp:Mdp):\n",
    "        super().__init__(mdp)\n",
    "\n",
    "    def get_action(self, state):\n",
    "        return np.random.randint(self.mdp.nb_actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rQYX6vzUaR-j"
   },
   "source": [
    "Implement a simple policy that throws a die only if the score is below 16."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zBiltIc7aZqF"
   },
   "outputs": [],
   "source": [
    "class ThresholdPolicy(Policy):\n",
    "    def __init__(self, mdp):\n",
    "        super().__init__(mdp)\n",
    "\n",
    "    def get_action(self, state):\n",
    "        # TODO: Return 1 (throw) if state < 15, else 0 (stop)\n",
    "        raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hHQtIZcXS0jK"
   },
   "outputs": [],
   "source": [
    "pol_unif = Uniform(blackjack_dice).get_action\n",
    "# Let us check the average performance of the random policy on 100 blackjack with dice games.\n",
    "policy_unif_eval = []\n",
    "for episodes in range(100):\n",
    "    policy_unif_eval.append(blackjack_dice.one_episode(pol_unif))\n",
    "\n",
    "pol_thresh = ThresholdPolicy(blackjack_dice).get_action\n",
    "# Let us check the average performance of the threshold policy on 100 blackjack with dice games.\n",
    "policy_thresh_eval = []\n",
    "for episodes in range(100):\n",
    "    policy_thresh_eval.append(blackjack_dice.one_episode(pol_thresh))\n",
    "\n",
    "\n",
    "print(\"mean +- std of the score of the uniform policy: {} +/- {}\".format(np.mean(policy_unif_eval), np.std(policy_unif_eval)))\n",
    "print(\"mean +- std of the score of the threshold policy: {} +/- {}\".format(np.mean(policy_thresh_eval), np.std(policy_thresh_eval)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VB6bjWDR06fq"
   },
   "source": [
    "# 2. Dynamic Programming\n",
    "\n",
    "## 2.1 State value function $V^{\\pi}(s)$\n",
    "\n",
    "One way to find the optimal policy for a MDP is to find the states with highest value, then $\\pi^*$ is the policy that visits those states. The value of a state, given a policy $\\pi$, is the expected cumulative reward obtained by $\\pi$ over MDP episodes when starting the episodes in $s_0 = s$. It is defined as follows with the Bellman expectation equation:\n",
    "\n",
    "$V^{\\pi}(s) = \\underset{s'\\in S}{\\sum} P(s'|s, \\pi(s))(R(s, \\pi(s), s') + \\gamma V^{\\pi}(s'))$\n",
    "\n",
    "The value of state $s$ is simply the reward obtained by $\\pi$ in state $s$, plus the value of the expected next state $s'$.\n",
    "\n",
    "## 2.2 Exercise 3: Implement the Value Iteration algorithm\n",
    "\n",
    "The Bellman optimality equation defines an algorithm to find the value function associated with the optimal policy.\n",
    "\n",
    "$V^*(s) = \\underset{a}{\\operatorname{max}}\\left [ \\underset{s'\\in S}{\\sum} P(s'|s, a)(R(s, a, s') + \\gamma V^{*}(s'))\\right]$\n",
    "\n",
    "[The Value Iteration algorithm](http://incompleteideas.net/book/ebook/node44.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UWIUK0Z-NN0Z"
   },
   "outputs": [],
   "source": [
    "def VI(mdp, eps=1e-2):\n",
    "    \"\"\"Value Iteration algorithm\"\"\"\n",
    "    V = np.zeros(mdp.nb_states)\n",
    "    list_error = []\n",
    "\n",
    "    converged = False\n",
    "    while not converged:\n",
    "        V_old = V.copy()\n",
    "\n",
    "        for s in mdp.states:\n",
    "            if s in mdp.terminal_states:\n",
    "                continue\n",
    "\n",
    "            # TODO: Compute value for each action\n",
    "            raise NotImplementedError\n",
    "\n",
    "        # Check convergence\n",
    "        error = np.linalg.norm(V - V_old)\n",
    "        list_error.append(error)\n",
    "        if error < eps:\n",
    "            converged = True\n",
    "\n",
    "    return V, list_error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p6-gLcQEftii"
   },
   "source": [
    "Run VI on the blackjack with dice MDP and plot the convergence of the algorithm for different thresholds $\\epsilon$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9hARYJBXgBuY"
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "V, list_error = VI(blackjack_dice, eps = 0.001)\n",
    "print(V)\n",
    "plt.plot(list_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F8G1lpgY92ss"
   },
   "source": [
    "You can visualize the optimal Value function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VMgYba5f97NI"
   },
   "outputs": [],
   "source": [
    "tmp = V.reshape(-1,1)\n",
    "plt.imshow(tmp.T)\n",
    "plt.xlabel(\"$s$\")\n",
    "plt.ylabel(\"$V^*(s)$\")\n",
    "plt.yticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4kJ--Hc5WEpo"
   },
   "source": [
    "## 2.3 Exercise 4: Get the optimal policy $\\pi^*$ from $V^*$\n",
    "\n",
    "Implement the class PolicyFromV. It takes as input an MDP and the associated $V^*$. In state $s$, $\\pi^*(s)$ should return the action $a$ that leads to the next state $s'$ with highest value $V^*(s')$.\n",
    "\n",
    "$\\pi^*(s) = \\underset{a}{\\operatorname{argmax}}\\left [ \\underset{s'\\in S}{\\sum} P(s'|s, a)(R(s, a, s') + \\gamma V^{*}(s'))\\right]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ah81UT3fUFOa"
   },
   "outputs": [],
   "source": [
    "class PolicyFromV(Policy):\n",
    "    def __init__(self, mdp, V):\n",
    "        super().__init__(mdp)\n",
    "        self.V = V\n",
    "        self.get_policy()\n",
    "\n",
    "    def get_policy(self):\n",
    "        self.policy = np.zeros(self.mdp.nb_states, dtype=int)\n",
    "\n",
    "        for s in self.mdp.states:\n",
    "            if s in self.mdp.terminal_states:\n",
    "                continue\n",
    "\n",
    "            # TODO: Compute value for each action\n",
    "            raise NotImplementedError\n",
    "\n",
    "    def get_action(self, state):\n",
    "        return int(self.policy[state])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZM_4gpWheeWV"
   },
   "source": [
    "Compare the performances of $\\pi^*$ with $\\pi^{\\text{unif}}$ and $\\pi^{\\text{thresh}}$ on 100 blackjack with dice games."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oh8XbJysevc8"
   },
   "outputs": [],
   "source": [
    "pol_opt = PolicyFromV(blackjack_dice, V).get_action\n",
    "# Evaluate optimal policy on 100 episodes\n",
    "# TODO\n",
    "raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pNhciod9hR17"
   },
   "source": [
    "**Question**: Interpret your policy (plot the policy using imshow). Can you explain how your policy $\\pi^*$ plays blackjack with dice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p7Sr83tLimBL"
   },
   "source": [
    "## 2.4 State-action values $Q^{\\pi}(s,a)$\n",
    "\n",
    "Similarly to $V^{\\pi}(s)$ it is possible to define the state-action value of $s,a$ when following policy $\\pi$. The state-action value $Q^{\\pi}(s,a)$ is the expected cumulative reward obtained by $\\pi$ over MDP episodes when starting the episodes in $s_0 = s$ and choosing $a_0 = a$.\n",
    "\n",
    "$Q^{\\pi}(s,a) = \\underset{s'\\in S}{\\sum} P(s'|s, a)(R(s, a, s') + \\gamma V^{\\pi}(s'))$\n",
    "\n",
    "We also have:\n",
    "\n",
    "$Q^*(s,a) = \\underset{s'\\in S}{\\sum} P(s'|s, a)(R(s, a, s') + \\gamma{\\operatorname{max}}Q^*(s',a'))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mUJkdPRFfWgK"
   },
   "source": [
    "# 3. Reinforcement Learning\n",
    "\n",
    "By contrast with dynamic programming, a reinforcement learning agent is used when the MDP is unknown. More precisely, the state and action are known, but the agent does not know the transition nor the reward function.\n",
    "\n",
    "## 3.1 Q-Learning\n",
    "\n",
    "The Q-learning algorithm learns an estimate $\\hat{Q}$ of $Q^*$. It does so without an MDP transition probability model: it is a model-free RL algorithm. Q-learning collects transitions $(s,a,r,s')$ following a behaviour policy $\\pi^B$. For each collected transition, $\\hat{Q}$ is updated as follows:\n",
    "\n",
    "$\\hat{Q}(s,a) \\leftarrow \\hat{Q}(s,a) + \\alpha \\delta$\n",
    "\n",
    "Where $\\delta$ is the temporal difference error:\n",
    "\n",
    "$r + \\gamma \\underset{a'}{\\operatorname{max}}\\hat{Q}(s',a') - \\hat{Q}(s,a)$\n",
    "\n",
    "The behaviour policy trades-off exploration and exploitation when collecting transitions in the MDP. For example, $\\pi^B$ can be the epsilon-greedy policy:\n",
    "\n",
    "$\\pi^\\epsilon(s) = \\pi^{\\text{unif}}$, with probability $\\epsilon$.\n",
    "\n",
    "$\\pi^\\epsilon(s) = \\underset{a}{\\operatorname{argmax}}\\hat{Q}(s, a)$, otherwise.\n",
    "\n",
    "## 3.2 Exercise 5: Implement the Q-Learning algorithm\n",
    "\n",
    "[Q-Learning algorithm](http://incompleteideas.net/book/ebook/node65.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZnjXuTUw89oM"
   },
   "outputs": [],
   "source": [
    "def QLearning(mdp, behaviour_pol, alpha=0.1, nb_iter=int(1e4)):\n",
    "    \"\"\"Q-Learning algorithm\"\"\"\n",
    "    Qhat = np.zeros((mdp.nb_states, mdp.nb_actions))\n",
    "    list_Qs = []\n",
    "    s = mdp.reset()\n",
    "\n",
    "    for _ in range(nb_iter):\n",
    "        a = behaviour_pol(s, Qhat)\n",
    "        s_next, r, done = mdp.step(a)\n",
    "\n",
    "        if done:\n",
    "            Qhat[s, a] = r\n",
    "            s = mdp.reset()\n",
    "        else:\n",
    "            # TODO: Implement TD update\n",
    "            raise NotImplementedError\n",
    "\n",
    "            list_Qs.append(Qhat.copy())\n",
    "            s = s_next\n",
    "\n",
    "    return Qhat, list_Qs\n",
    "\n",
    "class EpsGreedyPolicy(Policy):\n",
    "    def __init__(self, mdp, epsilon):\n",
    "        self.mdp = mdp\n",
    "        self.eps = epsilon\n",
    "\n",
    "    def get_action(self, state, Q):\n",
    "        if np.random.random() < self.eps:\n",
    "            return np.random.randint(self.mdp.nb_actions)\n",
    "        else:\n",
    "            return np.argmax(Q[state, :])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fyd5P8UfCZ3L"
   },
   "source": [
    "Try Q-Learning to learn an estimate of the optimal state-action value function of the blackjack with dice MDP.\n",
    "\n",
    "- Visualize $\\hat{Q}$ returned by Q-Learning (use $\\texttt{plt.imshow()}$). Does this Q function make sense?\n",
    "- Make a plot to compare the convergence of $\\hat{Q}$ over iterations for different values of $\\epsilon$ and different learning rates $\\alpha$.\n",
    "- Compare the performance of the greedy policy $\\pi^{\\text{greedy}}(s) = \\underset{a}{\\operatorname{argmax}}\\hat{Q}(s, a)$ with other policies (uniform, threshold, $\\pi^*$) on 100 episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ANQxsvF3ELTF"
   },
   "outputs": [],
   "source": [
    "pol_behaviour = EpsGreedyPolicy(blackjack_dice, epsilon=0.3).get_action\n",
    "q, list_Qs= QLearning(blackjack_dice, pol_behaviour)\n",
    "# TODO\n",
    "raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FIL-xYROmRhm"
   },
   "source": [
    "# 4. Homework: Maze MDPs and Reinforcement Learning\n",
    "\n",
    "This homework is open to interpretation and implementation. You can work in pairs.\n",
    "\n",
    "**Deadline**: December 3rd, 2025. Send your report to [thomas.michel@inria.fr](mailto:thomas.michel@inria.fr)\n",
    "\n",
    "### Exercise 1: Implement a Maze MDP Generator\n",
    "\n",
    "Mazes are often used to benchmark new RL algorithms. One can think of a maze MDP as a rectangle divided into cells. Each cell is a MDP state. The upper left cell is the initial state and the bottom right is the exit. Walls should be placed at random in the MDP: this will be done by specifying a fraction of cell states that are non-accessible. There are 4 actions in a maze MDP: left, right, up, down. All state-action pairs give a 0 reward, except a state-action pair that leads to the exit (terminal) state in the bottom right cell.\n",
    "\n",
    "- How to make sure that a generated maze can be exited? You can try checking $V^*(s_{start}) > 0$ for instance.\n",
    "- Your Maze MDP generator should take as arguments: a width, a height, and a fraction of cell walls. $|S| = \\text{width} \\times \\text{height}$.\n",
    "\n",
    "### Exercise 2: 3-Page Report on Q-Learning Performance\n",
    "\n",
    "Present the performances of Q-Learning on maze MDPs:\n",
    "\n",
    "- Check that your learned Q functions make sense on some examples.\n",
    "- Compare eps-greedy policy and softmax behaviour policy.\n",
    "- Try changing the parameters of those policies, as well as the size and difficulty of the maze MDPs. What can you observe? Can you explain it?\n",
    "- Can you suggest/implement improvements to the algorithm?\n",
    "\n",
    "**Note**: In general, describe your experiments and do not put too many figures (choose a few convincing figures) and then describe what you observe and try to understand if it makes sense."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
