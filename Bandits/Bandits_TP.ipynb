{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn.datasets as datasets\n",
    "from sklearn.model_selection import train_test_split as split\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Multi-Armed Bandit Problems\n",
    "We study the classical multi-armed bandit problem specified by a set of real-valued\n",
    "distributions $ (\\nu_a)_{a \\in \\mathcal{A}}$ with means $(\\mu_a)_{a \\in \\mathcal{A}} \\in \\mathbb{R}^\\mathcal{A}$, where $\\mathcal{A}$ is a finite set of arms.\n",
    "## 1.1 Bandit Arms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A generic bandit arm class has one\n",
    "# method to sample a reward from a probability distribution\n",
    "\n",
    "class Arm:\n",
    "    def __init__(self, mean: float):\n",
    "        self.mean = mean\n",
    "        pass\n",
    "    def sample(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "# For example, a Bernoulli bandit arm looks like this\n",
    "class BernoulliArm(Arm):\n",
    "    def __init__(self, p:float):\n",
    "        # create a Bernoulli arm with mean p\n",
    "        super().__init__(mean=p)\n",
    "\n",
    "    def sample(self):\n",
    "        # generate a reward from a Bernoulli arm\n",
    "        return np.random.random() < self.mean\n",
    "\n",
    "class ExponentialArm(Arm):\n",
    "    def __init__(self, beta:float):\n",
    "        super().__init__(mean=beta)\n",
    "\n",
    "    def sample(self):\n",
    "        return np.random.exponential(scale=self.mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Exercise 1: Implement a bandit arm class with Gaussian distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianArm(Arm):\n",
    "    def __init__(self, mean: float, var: float):\n",
    "        super().__init__(mean=mean)\n",
    "        self.var = var\n",
    "\n",
    "    def sample(self):\n",
    "        # TODO: Exercise 1\n",
    "        raise NotImplementedError(\"Exercise 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 A Multi-Armed Bandit (MAB)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now write a generic MAB problem.\n",
    "# When pulling arm K of a bandit problem, a learning agent gets an instantinuous\n",
    "# reward and a regret value that is the difference between this reward and the best\n",
    "# mean over all arms.\n",
    "\n",
    "class MAB:\n",
    "    def __init__(self, arms: list[Arm]):\n",
    "        self.arms = arms\n",
    "        # We compute the max over arms means for regret computations\n",
    "        self.best_arm_mean = max([arm.mean for arm in self.arms])\n",
    "\n",
    "    def sample(self, arm_number: int):\n",
    "        reward = self.arms[arm_number].sample()\n",
    "        regret = self.best_arm_mean - reward\n",
    "        return reward, regret\n",
    "\n",
    "# A bandit problem with two exponential arms and one Bernoulli arm\n",
    "test_mab = MAB([ExponentialArm(beta=0.3), ExponentialArm(beta=0.1), BernoulliArm(p=0.7)])\n",
    "reward, regret = test_mab.sample(arm_number=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Exercise 2: Implement a simple MAB class where all the arms are Bernoulli distributed. Instantiate this class by passing a list of means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MABBernoulli(MAB):\n",
    "    def __init__(self, list_means: list[float]):\n",
    "        # TODO: Exercise 2\n",
    "        raise NotImplementedError(\"Exercise 2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 Cumultative Regret Minimization\n",
    "At each time $t \\geq 1$, a learner must choose an arm $a_t \\in \\mathcal{A}$, based only on\n",
    "the past. The learner then receives and observes a reward $X_t$ sampled according to $\\nu_{a_t}$. The goal of the learner is simply to maximize the expected sum of rewards received over time, or equivalently minimize regret\n",
    "with respect to the strategy constantly receiving the highest mean reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bandit algorithms learn arm pulling strategies to minimize the cumulative regret over T pulls.\n",
    "def cumul_regret(regrets: list[float]):\n",
    "    return np.cumsum(regrets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Bandit Algorithms\n",
    "## 2.1 Uniform Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A bandit algorithm has one key component which is the sampling strategy (which arm to pull ater T pulls).\n",
    "\n",
    "class BanditAlgo:\n",
    "    def __init__(self, mab: MAB):\n",
    "        self.mab = mab\n",
    "        self.rewards = []\n",
    "        self.regrets = []\n",
    "        self.arms_drawn = []\n",
    "\n",
    "    def sampling(self, timesteps_T: int):\n",
    "        # Where to implement the sampling strategies\n",
    "        raise NotImplementedError\n",
    "\n",
    "# The most naive bandit algorithm is to draw arms at random.\n",
    "class UniformSamplingAlgo(BanditAlgo):\n",
    "    def __init__(self, mab: MAB):\n",
    "        super().__init__(mab=mab)\n",
    "\n",
    "    def sampling(self, timesteps_T: int):\n",
    "        for i in range(timesteps_T):\n",
    "            arm_to_pull = np.random.randint(0, len(self.mab.arms))\n",
    "            reward, regret = self.mab.sample(arm_to_pull)\n",
    "            self.rewards.append(reward)\n",
    "            self.regrets.append(regret)\n",
    "            self.arms_drawn.append(arm_to_pull)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other naive algorithms include Follow the Leader and Explore then Commit. They\n",
    "# both sample each arm at least once before pulling the best arm for the rest of\n",
    "# the time.\n",
    "class FollowTheLeader(BanditAlgo):\n",
    "    def __init__(self, mab: MAB):\n",
    "        super().__init__(mab=mab)\n",
    "\n",
    "    def sampling(self, timesteps_T: int):\n",
    "        if len(self.mab.arms) > timesteps_T:\n",
    "            print(\"Warning: more arms than timesteps\")\n",
    "\n",
    "        # Pull each arm once\n",
    "        arm_rewards = [[] for _ in range(len(self.mab.arms))]\n",
    "        for i in range(len(self.mab.arms)):\n",
    "            reward, regret = self.mab.sample(i)\n",
    "            self.rewards.append(reward)\n",
    "            self.regrets.append(regret)\n",
    "            self.arms_drawn.append(i)\n",
    "            arm_rewards[i].append(reward)\n",
    "\n",
    "        # Get the best arm based on empirical means\n",
    "        empirical_means = [np.mean(rewards) for rewards in arm_rewards]\n",
    "        best_arm = np.argmax(empirical_means)\n",
    "\n",
    "        # for the rest of the time, pull the best arm\n",
    "        for j in range(len(self.mab.arms), timesteps_T):\n",
    "            reward, regret = self.mab.sample(best_arm)\n",
    "            self.rewards.append(reward)\n",
    "            self.regrets.append(regret)\n",
    "            self.arms_drawn.append(best_arm)\n",
    "\n",
    "class ExploreThenCommit(BanditAlgo):\n",
    "    def __init__(self, mab: MAB, explore_fraction: float):\n",
    "        super().__init__(mab=mab)\n",
    "        self.explore_fraction = explore_fraction\n",
    "\n",
    "    def sampling(self, timesteps_T: int):\n",
    "        explore_steps = int(timesteps_T * self.explore_fraction)\n",
    "        arm_rewards = [[] for _ in range(len(self.mab.arms))]\n",
    "        \n",
    "        for i in range(explore_steps):\n",
    "            arm_to_pull = np.random.randint(0, len(self.mab.arms))\n",
    "            reward, regret = self.mab.sample(arm_to_pull)\n",
    "            self.rewards.append(reward)\n",
    "            self.regrets.append(regret)\n",
    "            self.arms_drawn.append(arm_to_pull)\n",
    "            arm_rewards[arm_to_pull].append(reward)\n",
    "\n",
    "        # Get the best arm based on empirical means\n",
    "        empirical_means = [np.mean(rewards) if len(rewards) > 0 else -np.inf for rewards in arm_rewards]\n",
    "        best_arm = np.argmax(empirical_means)\n",
    "\n",
    "        # for the rest of the time, pull the best arm\n",
    "        for j in range(explore_steps, timesteps_T):\n",
    "            reward, regret = self.mab.sample(best_arm)\n",
    "            self.rewards.append(reward)\n",
    "            self.regrets.append(regret)\n",
    "            self.arms_drawn.append(best_arm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A plotting function\n",
    "def plot_mean_std_cumul_regret(regret_matrix: list[list[float]], algo_name: str):\n",
    "    means = regret_matrix.mean(axis=0)\n",
    "    stds = regret_matrix.std(axis=0)\n",
    "    plt.plot(means, label=algo_name)\n",
    "    plt.fill_between(np.arange(len(means)), means - stds, means + stds, alpha = 0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of Uniform Sampling algo on a two-armed Bernoulli bandit\n",
    "\n",
    "# MAB problem\n",
    "mab = MAB([BernoulliArm(p=0.2), BernoulliArm(p=0.6)])\n",
    "# Experimental Setup\n",
    "repetitions = 1000\n",
    "timesteps_per_repet = 100\n",
    "cum_regrets = np.zeros((repetitions, timesteps_per_repet))\n",
    "\n",
    "for rep in range(repetitions):\n",
    "    unif = UniformSamplingAlgo(mab)\n",
    "    unif.sampling(timesteps_per_repet)\n",
    "    cum_regrets[rep] = cumul_regret(unif.regrets)\n",
    "plot_mean_std_cumul_regret(cum_regrets, 'uniform')\n",
    "plt.legend()\n",
    "plt.xlabel(\"T\")\n",
    "plt.ylabel(\"Cumulative Regret\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Exercise 3: Plot the cumulative regret of bandit algorithms Uniform, FTL, and ETC on a Bernoulli MAB problem. Try different exploration rates for ETC.\n",
    "Try different MAB Problems (try one with gaussian arms, that have high variance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate(mab, algo, timesteps_per_repet, repetitions, **kwargs):\n",
    "    cum_regrets = np.zeros((repetitions, timesteps_per_repet))\n",
    "    for rep in range(repetitions):\n",
    "        exp = algo(mab, **kwargs)\n",
    "        exp.sampling(timesteps_per_repet)\n",
    "        cum_regrets[rep] = cumul_regret(exp.regrets)\n",
    "    return cum_regrets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Exercise 3\n",
    "raise NotImplementedError(\"Exercise 3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Advanced Bandit Algorithms\n",
    "## 3.1 Exercise 4: Implement the Upper Confidence Bounds algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UpperConfBounds(BanditAlgo):\n",
    "    def __init__(self, mab: MAB):\n",
    "        super().__init__(mab=mab)\n",
    "        self.Qs = [[] for arm in self.mab.arms]\n",
    "        self.Ns = [0 for arm in self.mab.arms]\n",
    "\n",
    "    def sampling(self, timesteps_T: int):\n",
    "        # TODO: Exercise 4\n",
    "        raise NotImplementedError(\"Exercise 4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Exercise 5: Implement the Thompson Sampling algorithm (TS) with Beta prior and Bernoulli likelihood\n",
    "For all arm $a \\in \\mathcal{A}$, for all time step  $t \\geq 1$, the cumulative reward from arm $a$ at time $t$ is $$S_a(t) = \\sum\\limits_{s = 1}^t \\mathbb{1}_{\\{a_s = a\\}}X_s\\,, $$\n",
    "and the number of pulls of arm $a$ at time $t$ is\n",
    "$$N_a(t) = \\sum\\limits_{ s=1}^t \\mathbb{1}_{\\{ a_s = a\\}} \\,.$$\n",
    "\n",
    "In a Bayesian view on the MAB, the $(\\mu_a)_{a\\in\\mathcal{A}}$ are no longer seen as unknown parameters but as (independent)random variables following a uniform distribution. The posterior distribution on the arm $a$ at time $t$ of the bandit game is the distribution of $\\mu_a$ conditional to the observations from arm $a$ gathered up to\n",
    "time $t$ and it is denoted $\\pi_a(t)$ . Each sample from arm $a$ leads to an update of this posterior distribution.\n",
    "\n",
    "\n",
    "TS is the strategy that consists in drawing $\\theta_a(t)\\sim \\pi_a(t)=Beta(N_a(t) + 1, N_a(t) - S_a(t) + 1)$ at time step t and for each arm $a$, then pulling the arm:\n",
    "$$ a_{t+1} = \\arg\\!\\max_{a \\in \\mathcal{A}}\\theta_a(t)  \\,.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThompsonSampling(BanditAlgo):\n",
    "    def __init__(self, mab: MAB):\n",
    "        super().__init__(mab=mab)\n",
    "        self.S = [0 for _ in range(len(self.mab.arms))]\n",
    "        self.N = [0 for _ in range(len(self.mab.arms))]\n",
    "\n",
    "    def sampling(self, timesteps_T: int):\n",
    "        # TODO: Exercise 5\n",
    "        raise NotImplementedError(\"Exercise 5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare Thompson Sampling with other algorithms on Bernoulli bandits\n",
    "# TODO\n",
    "raise NotImplementedError(\"TODO\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
